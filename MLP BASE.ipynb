{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff4d1ae-2732-41ee-958c-3d61a2eacbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c21dc0c-8a91-4825-b2ad-86aeb44dab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, features_file, label_file):\n",
    "        self.features = pd.read_csv(features_file)\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float)\n",
    "        tensor_labels = torch.tensor(self.labels.iloc[idx].values, dtype=torch.float)\n",
    "\n",
    "        return tensor_features, tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd606a64-51c8-4378-a9c5-dc56609aba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('H1_AI_Dataset/H1_Features_Wh.csv', 'H1_AI_Dataset/H1_Labels_Wh.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fce46-4ffb-4104-ad7e-a0ae1d9f29ec",
   "metadata": {},
   "source": [
    "## Below we investigate the first batch of Features & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1ad3c3-b434-4fca-86db-96eb8bcc0c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " tensor([[ 35.6726,  43.5336,  23.1728,  23.7152,  24.6601,  35.2410,  23.5270,\n",
      "          34.5210],\n",
      "        [ 43.5336,  23.1728,  23.7152,  24.6601,  35.2410,  23.5270,  34.5210,\n",
      "          43.5534],\n",
      "        [ 23.1728,  23.7152,  24.6601,  35.2410,  23.5270,  34.5210,  43.5534,\n",
      "          40.9924],\n",
      "        [ 23.7152,  24.6601,  35.2410,  23.5270,  34.5210,  43.5534,  40.9924,\n",
      "          28.8335],\n",
      "        [ 24.6601,  35.2410,  23.5270,  34.5210,  43.5534,  40.9924,  28.8335,\n",
      "          23.1566],\n",
      "        [ 35.2410,  23.5270,  34.5210,  43.5534,  40.9924,  28.8335,  23.1566,\n",
      "          33.5603],\n",
      "        [ 23.5270,  34.5210,  43.5534,  40.9924,  28.8335,  23.1566,  33.5603,\n",
      "          26.3208],\n",
      "        [ 34.5210,  43.5534,  40.9924,  28.8335,  23.1566,  33.5603,  26.3208,\n",
      "          25.9587],\n",
      "        [ 43.5534,  40.9924,  28.8335,  23.1566,  33.5603,  26.3208,  25.9587,\n",
      "          42.8160],\n",
      "        [ 40.9924,  28.8335,  23.1566,  33.5603,  26.3208,  25.9587,  42.8160,\n",
      "          33.9373],\n",
      "        [ 28.8335,  23.1566,  33.5603,  26.3208,  25.9587,  42.8160,  33.9373,\n",
      "          23.1287],\n",
      "        [ 23.1566,  33.5603,  26.3208,  25.9587,  42.8160,  33.9373,  23.1287,\n",
      "          22.8041],\n",
      "        [ 33.5603,  26.3208,  25.9587,  42.8160,  33.9373,  23.1287,  22.8041,\n",
      "          26.2109],\n",
      "        [ 26.3208,  25.9587,  42.8160,  33.9373,  23.1287,  22.8041,  26.2109,\n",
      "          33.6147],\n",
      "        [ 25.9587,  42.8160,  33.9373,  23.1287,  22.8041,  26.2109,  33.6147,\n",
      "          23.1812],\n",
      "        [ 42.8160,  33.9373,  23.1287,  22.8041,  26.2109,  33.6147,  23.1812,\n",
      "          46.0127],\n",
      "        [ 33.9373,  23.1287,  22.8041,  26.2109,  33.6147,  23.1812,  46.0127,\n",
      "          56.3590],\n",
      "        [ 23.1287,  22.8041,  26.2109,  33.6147,  23.1812,  46.0127,  56.3590,\n",
      "          32.5830],\n",
      "        [ 22.8041,  26.2109,  33.6147,  23.1812,  46.0127,  56.3590,  32.5830,\n",
      "          23.4713],\n",
      "        [ 26.2109,  33.6147,  23.1812,  46.0127,  56.3590,  32.5830,  23.4713,\n",
      "          23.0706],\n",
      "        [ 33.6147,  23.1812,  46.0127,  56.3590,  32.5830,  23.4713,  23.0706,\n",
      "         115.1823],\n",
      "        [ 23.1812,  46.0127,  56.3590,  32.5830,  23.4713,  23.0706, 115.1823,\n",
      "         220.1092],\n",
      "        [ 46.0127,  56.3590,  32.5830,  23.4713,  23.0706, 115.1823, 220.1092,\n",
      "         109.2542],\n",
      "        [ 56.3590,  32.5830,  23.4713,  23.0706, 115.1823, 220.1092, 109.2542,\n",
      "         103.4470],\n",
      "        [ 32.5830,  23.4713,  23.0706, 115.1823, 220.1092, 109.2542, 103.4470,\n",
      "          74.8763],\n",
      "        [ 23.4713,  23.0706, 115.1823, 220.1092, 109.2542, 103.4470,  74.8763,\n",
      "          65.9099],\n",
      "        [ 23.0706, 115.1823, 220.1092, 109.2542, 103.4470,  74.8763,  65.9099,\n",
      "          76.6463],\n",
      "        [115.1823, 220.1092, 109.2542, 103.4470,  74.8763,  65.9099,  76.6463,\n",
      "          80.4577],\n",
      "        [220.1092, 109.2542, 103.4470,  74.8763,  65.9099,  76.6463,  80.4577,\n",
      "          91.9805],\n",
      "        [109.2542, 103.4470,  74.8763,  65.9099,  76.6463,  80.4577,  91.9805,\n",
      "          76.4533],\n",
      "        [103.4470,  74.8763,  65.9099,  76.6463,  80.4577,  91.9805,  76.4533,\n",
      "          89.9678],\n",
      "        [ 74.8763,  65.9099,  76.6463,  80.4577,  91.9805,  76.4533,  89.9678,\n",
      "          64.8212],\n",
      "        [ 65.9099,  76.6463,  80.4577,  91.9805,  76.4533,  89.9678,  64.8212,\n",
      "         142.9364],\n",
      "        [ 76.6463,  80.4577,  91.9805,  76.4533,  89.9678,  64.8212, 142.9364,\n",
      "          36.5406],\n",
      "        [ 80.4577,  91.9805,  76.4533,  89.9678,  64.8212, 142.9364,  36.5406,\n",
      "          58.4747],\n",
      "        [ 91.9805,  76.4533,  89.9678,  64.8212, 142.9364,  36.5406,  58.4747,\n",
      "          52.9815],\n",
      "        [ 76.4533,  89.9678,  64.8212, 142.9364,  36.5406,  58.4747,  52.9815,\n",
      "          55.1669],\n",
      "        [ 89.9678,  64.8212, 142.9364,  36.5406,  58.4747,  52.9815,  55.1669,\n",
      "          43.0476],\n",
      "        [ 64.8212, 142.9364,  36.5406,  58.4747,  52.9815,  55.1669,  43.0476,\n",
      "          39.0584],\n",
      "        [142.9364,  36.5406,  58.4747,  52.9815,  55.1669,  43.0476,  39.0584,\n",
      "          52.6394],\n",
      "        [ 36.5406,  58.4747,  52.9815,  55.1669,  43.0476,  39.0584,  52.6394,\n",
      "          58.9725],\n",
      "        [ 58.4747,  52.9815,  55.1669,  43.0476,  39.0584,  52.6394,  58.9725,\n",
      "          44.8363],\n",
      "        [ 52.9815,  55.1669,  43.0476,  39.0584,  52.6394,  58.9725,  44.8363,\n",
      "          38.5841],\n",
      "        [ 55.1669,  43.0476,  39.0584,  52.6394,  58.9725,  44.8363,  38.5841,\n",
      "          50.5557],\n",
      "        [ 43.0476,  39.0584,  52.6394,  58.9725,  44.8363,  38.5841,  50.5557,\n",
      "          40.9163],\n",
      "        [ 39.0584,  52.6394,  58.9725,  44.8363,  38.5841,  50.5557,  40.9163,\n",
      "          37.8877],\n",
      "        [ 52.6394,  58.9725,  44.8363,  38.5841,  50.5557,  40.9163,  37.8877,\n",
      "          56.5023],\n",
      "        [ 58.9725,  44.8363,  38.5841,  50.5557,  40.9163,  37.8877,  56.5023,\n",
      "          72.9588],\n",
      "        [ 44.8363,  38.5841,  50.5557,  40.9163,  37.8877,  56.5023,  72.9588,\n",
      "          39.5194],\n",
      "        [ 38.5841,  50.5557,  40.9163,  37.8877,  56.5023,  72.9588,  39.5194,\n",
      "          27.5422],\n",
      "        [ 50.5557,  40.9163,  37.8877,  56.5023,  72.9588,  39.5194,  27.5422,\n",
      "          30.3922],\n",
      "        [ 40.9163,  37.8877,  56.5023,  72.9588,  39.5194,  27.5422,  30.3922,\n",
      "          42.1133],\n",
      "        [ 37.8877,  56.5023,  72.9588,  39.5194,  27.5422,  30.3922,  42.1133,\n",
      "          39.4536],\n",
      "        [ 56.5023,  72.9588,  39.5194,  27.5422,  30.3922,  42.1133,  39.4536,\n",
      "          50.6995],\n",
      "        [ 72.9588,  39.5194,  27.5422,  30.3922,  42.1133,  39.4536,  50.6995,\n",
      "          30.9019],\n",
      "        [ 39.5194,  27.5422,  30.3922,  42.1133,  39.4536,  50.6995,  30.9019,\n",
      "         129.1429],\n",
      "        [ 27.5422,  30.3922,  42.1133,  39.4536,  50.6995,  30.9019, 129.1429,\n",
      "          53.1020],\n",
      "        [ 30.3922,  42.1133,  39.4536,  50.6995,  30.9019, 129.1429,  53.1020,\n",
      "          45.7269],\n",
      "        [ 42.1133,  39.4536,  50.6995,  30.9019, 129.1429,  53.1020,  45.7269,\n",
      "         102.5284],\n",
      "        [ 39.4536,  50.6995,  30.9019, 129.1429,  53.1020,  45.7269, 102.5284,\n",
      "         126.1336],\n",
      "        [ 50.6995,  30.9019, 129.1429,  53.1020,  45.7269, 102.5284, 126.1336,\n",
      "         165.7835],\n",
      "        [ 30.9019, 129.1429,  53.1020,  45.7269, 102.5284, 126.1336, 165.7835,\n",
      "          83.6449],\n",
      "        [129.1429,  53.1020,  45.7269, 102.5284, 126.1336, 165.7835,  83.6449,\n",
      "         171.7810],\n",
      "        [ 53.1020,  45.7269, 102.5284, 126.1336, 165.7835,  83.6449, 171.7810,\n",
      "         504.3534]])\n",
      "Labels:\n",
      " tensor([[2.6167e-01, 7.9889e-01, 4.9778e-01, 1.3046e+01, 4.3953e+00, 2.4600e+00],\n",
      "        [2.5778e-01, 7.3944e-01, 5.0167e-01, 2.2845e+01, 4.4803e+00, 2.6267e+00],\n",
      "        [2.5972e-01, 7.4778e-01, 4.9944e-01, 4.4028e+00, 4.3997e+00, 1.8285e+01],\n",
      "        [2.6167e-01, 7.4556e-01, 5.0056e-01, 9.9667e-01, 4.3553e+00, 7.5658e+00],\n",
      "        [2.5556e-01, 7.4167e-01, 4.9917e-01, 1.0019e+00, 4.4075e+00, 2.8467e+00],\n",
      "        [2.6083e-01, 7.3972e-01, 4.9917e-01, 1.3464e+01, 4.4500e+00, 3.0033e+00],\n",
      "        [2.5611e-01, 7.5000e-01, 5.0250e-01, 4.4883e+00, 4.4019e+00, 2.9400e+00],\n",
      "        [2.6083e-01, 7.4750e-01, 5.0000e-01, 3.8906e+00, 4.3464e+00, 2.8000e+00],\n",
      "        [2.6083e-01, 7.3944e-01, 4.9972e-01, 2.2831e+01, 4.3714e+00, 3.0000e+00],\n",
      "        [2.5694e-01, 7.4806e-01, 5.0167e-01, 1.3131e+01, 4.4217e+00, 2.9567e+00],\n",
      "        [2.5694e-01, 7.4333e-01, 4.9889e-01, 1.0022e+00, 1.0671e+01, 2.1867e+00],\n",
      "        [2.5972e-01, 7.5167e-01, 5.0194e-01, 1.0014e+00, 7.8792e+00, 2.8200e+00],\n",
      "        [2.5722e-01, 7.3778e-01, 4.9833e-01, 4.0200e+00, 7.9478e+00, 1.9033e+00],\n",
      "        [2.5250e-01, 7.3833e-01, 5.0056e-01, 1.2922e+01, 4.4767e+00, 1.9433e+00],\n",
      "        [2.5056e-01, 7.4833e-01, 4.9972e-01, 1.0025e+00, 4.4403e+00, 2.0600e+00],\n",
      "        [2.5361e-01, 7.4250e-01, 5.0389e-01, 9.8461e+00, 4.3136e+00, 1.8104e+01],\n",
      "        [2.7000e-01, 7.4167e-01, 5.0000e-01, 2.2522e+01, 4.3664e+00, 1.7729e+01],\n",
      "        [2.5222e-01, 7.4194e-01, 5.0056e-01, 8.4011e+00, 4.4625e+00, 6.8058e+00],\n",
      "        [2.7028e-01, 7.4750e-01, 5.0028e-01, 1.0053e+00, 4.4558e+00, 2.9933e+00],\n",
      "        [2.5583e-01, 7.4306e-01, 5.0083e-01, 9.9694e-01, 4.4992e+00, 2.5067e+00],\n",
      "        [1.1753e+00, 7.3972e-01, 9.9655e+01, 5.9169e+00, 4.2625e+00, 2.5433e+00],\n",
      "        [2.5278e-01, 7.3667e-01, 1.8791e+02, 1.1106e+01, 4.4658e+00, 1.9541e+01],\n",
      "        [4.5007e+01, 7.3250e-01, 2.9683e+01, 1.4766e+01, 4.3706e+00, 1.5786e+01],\n",
      "        [6.4784e+01, 7.3806e-01, 5.0000e-01, 2.2269e+01, 4.3500e+00, 1.6391e+01],\n",
      "        [3.0576e+01, 7.3944e-01, 5.0306e-01, 9.9806e-01, 4.2569e+00, 1.6314e+01],\n",
      "        [2.5583e-01, 7.4639e-01, 5.0056e-01, 1.0011e+00, 4.3978e+00, 1.7430e+01],\n",
      "        [2.5639e-01, 7.3806e-01, 5.0000e-01, 1.1853e+01, 4.1656e+00, 2.0352e+01],\n",
      "        [2.6389e-01, 7.4444e-01, 5.0056e-01, 1.8319e+01, 4.1694e+00, 1.8833e+01],\n",
      "        [2.6472e-01, 7.3750e-01, 5.0667e-01, 3.2097e+01, 4.1717e+00, 1.5204e+01],\n",
      "        [2.5806e-01, 7.4611e-01, 5.0056e-01, 1.2833e+01, 4.2353e+00, 1.7995e+01],\n",
      "        [2.5583e-01, 7.4722e-01, 4.9889e-01, 2.2614e+01, 4.1458e+00, 2.3449e+01],\n",
      "        [2.5333e-01, 7.3167e-01, 5.0139e-01, 2.3479e+01, 4.1494e+00, 2.2027e+01],\n",
      "        [2.5333e-01, 7.4750e-01, 1.0250e+02, 1.3505e+01, 4.1594e+00, 1.5331e+01],\n",
      "        [3.3881e+00, 7.5500e-01, 5.0444e-01, 1.2806e+00, 4.2242e+00, 1.5380e+01],\n",
      "        [1.0856e+01, 7.4250e-01, 5.0028e-01, 1.3934e+01, 4.1431e+00, 1.3769e+01],\n",
      "        [1.2383e+01, 1.2006e+00, 4.9944e-01, 2.3324e+01, 4.1183e+00, 2.5200e+00],\n",
      "        [1.2565e+01, 7.4861e-01, 5.0028e-01, 2.6159e+01, 4.1222e+00, 2.5400e+00],\n",
      "        [1.2561e+01, 7.4722e-01, 5.0139e-01, 1.0671e+01, 4.2042e+00, 3.6036e+00],\n",
      "        [7.9144e+00, 1.2614e+00, 4.9944e-01, 2.4333e+00, 4.1281e+00, 1.2754e+01],\n",
      "        [6.5775e+00, 1.2564e+00, 5.0250e-01, 1.5859e+01, 4.1206e+00, 1.5274e+01],\n",
      "        [6.4811e+00, 1.2531e+00, 5.0000e-01, 2.3461e+01, 4.1656e+00, 1.5055e+01],\n",
      "        [6.4886e+00, 1.2417e+00, 5.0611e-01, 6.7175e+00, 4.3053e+00, 1.5406e+01],\n",
      "        [6.4767e+00, 1.2431e+00, 5.0750e-01, 1.0028e+00, 4.1806e+00, 1.5024e+01],\n",
      "        [5.7836e+00, 1.0489e+00, 5.0111e-01, 1.3450e+01, 4.1011e+00, 1.3701e+01],\n",
      "        [4.7506e+00, 7.3056e-01, 5.0583e-01, 4.1578e+00, 4.1383e+00, 1.4236e+01],\n",
      "        [4.7567e+00, 7.4444e-01, 5.0028e-01, 9.9611e-01, 4.2461e+00, 1.4670e+01],\n",
      "        [4.7667e+00, 7.5222e-01, 4.9917e-01, 2.0374e+01, 4.1661e+00, 1.5069e+01],\n",
      "        [4.7997e+00, 7.4083e-01, 2.5586e+01, 2.2487e+01, 4.1322e+00, 1.1417e+01],\n",
      "        [4.7558e+00, 7.4444e-01, 1.0108e+01, 1.1017e+00, 4.1389e+00, 5.2739e+00],\n",
      "        [4.7750e+00, 7.4528e-01, 5.0056e-01, 1.0011e+00, 4.2425e+00, 2.7133e+00],\n",
      "        [4.7597e+00, 7.4667e-01, 5.0417e-01, 1.0033e+00, 4.1914e+00, 5.0647e+00],\n",
      "        [4.7836e+00, 7.4139e-01, 1.3207e+01, 1.6171e+01, 4.1567e+00, 3.0033e+00],\n",
      "        [4.8042e+00, 7.4889e-01, 5.0139e-01, 1.0844e+01, 4.1822e+00, 4.8817e+00],\n",
      "        [4.7744e+00, 1.5147e+00, 5.0333e-01, 2.2720e+01, 4.2369e+00, 4.7192e+00],\n",
      "        [4.7553e+00, 3.4686e+00, 4.9972e-01, 2.7133e+00, 4.1319e+00, 2.4967e+00],\n",
      "        [4.7858e+00, 7.4778e-01, 9.4771e+01, 1.2167e+00, 4.1089e+00, 1.7783e+01],\n",
      "        [4.7808e+00, 7.4611e-01, 4.9944e-01, 1.0672e+01, 4.1025e+00, 1.9549e+01],\n",
      "        [4.8144e+00, 7.3833e-01, 7.1744e+00, 7.3236e+00, 4.1817e+00, 9.5147e+00],\n",
      "        [8.2576e+01, 7.5028e-01, 4.9944e-01, 5.6072e+00, 4.0672e+00, 4.5242e+00],\n",
      "        [9.5574e+01, 7.5750e-01, 5.0167e-01, 2.3095e+01, 4.0975e+00, 2.3600e+00],\n",
      "        [9.5932e+01, 7.4861e-01, 5.1437e+01, 1.3112e+01, 4.0647e+00, 2.9000e+00],\n",
      "        [6.5960e+01, 7.3889e-01, 5.0083e-01, 1.0050e+00, 4.2000e+00, 3.8603e+00],\n",
      "        [9.3584e+01, 7.4722e-01, 8.1103e+01, 1.5658e+00, 4.1914e+00, 2.2667e+00],\n",
      "        [9.2052e+01, 7.3583e-01, 4.9972e-01, 4.3842e+02, 4.3214e+00, 2.0733e+00]])\n",
      "Shape of features: torch.Size([64, 8])\n",
      "Shape of labels: torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in dataloader:  # This correctly unpacks each batch into features and labels\n",
    "    print(\"Features:\\n\", features)\n",
    "    print(\"Labels:\\n\", labels)\n",
    "    print(\"Shape of features:\", features.shape)  \n",
    "    print(\"Shape of labels:\", labels.shape)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab23973-dbdb-4b8a-a3fd-6e35140e04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=128, output_size=6):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        \n",
    "        x = self.fc5(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261d4754-89f1-42f4-b8c6-e1525713908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, optimizer, and loss function\n",
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0b489c-907e-4db7-bc0a-622b958a45b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f804f1be-f32a-4c80-a02a-9c7a0ea224df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 8.9369\n",
      "Epoch [2/4], Loss: 8.8397\n",
      "Epoch [3/4], Loss: 8.7880\n",
      "Epoch [4/4], Loss: 8.7502\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b8eaf-9443-46a9-9e0c-f2845c69800e",
   "metadata": {},
   "source": [
    "## Below shows mean values for each column & total mean value in Labels\n",
    "* This is done to get an idea of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f8c7e3-1c99-4893-8ddb-b388cb2d3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise mean values of labels: tensor([ 9.4287,  7.4341, 16.7252, 22.2961,  4.6413,  6.2078])\n",
      "Scalar total mean of labels: tensor(11.1222)\n"
     ]
    }
   ],
   "source": [
    "total_sum = torch.zeros(6)  # Assuming there are 6 columns in your labels\n",
    "total_count = 0\n",
    "\n",
    "for _, labels in dataloader:\n",
    "    total_sum += labels.sum(dim=0)  # Sum each column across the batch\n",
    "    total_count += labels.size(0)  # Count the number of samples (rows)\n",
    "\n",
    "column_means = total_sum / total_count\n",
    "print(\"Column-wise mean values of labels:\", column_means)\n",
    "\n",
    "total_mean = column_means.sum() / column_means.numel()\n",
    "print(\"Scalar total mean of labels:\", total_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
