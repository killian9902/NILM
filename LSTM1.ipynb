{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e164af1-e3c2-40b5-a40c-4221b3744f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f6d881f-3e50-4971-866d-7516f32cc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, features_file, label_file):\n",
    "        self.features = pd.read_csv(features_file)\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float)\n",
    "        tensor_labels = torch.tensor(self.labels.iloc[idx].values, dtype=torch.float)\n",
    "\n",
    "        return tensor_features, tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ecb5744-09f2-4a24-9992-56e63ec8c5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " tensor([[0.0357, 0.0435, 0.0232, 0.0237, 0.0247, 0.0352, 0.0235, 0.0345],\n",
      "        [0.0435, 0.0232, 0.0237, 0.0247, 0.0352, 0.0235, 0.0345, 0.0436],\n",
      "        [0.0232, 0.0237, 0.0247, 0.0352, 0.0235, 0.0345, 0.0436, 0.0410],\n",
      "        [0.0237, 0.0247, 0.0352, 0.0235, 0.0345, 0.0436, 0.0410, 0.0288],\n",
      "        [0.0247, 0.0352, 0.0235, 0.0345, 0.0436, 0.0410, 0.0288, 0.0232],\n",
      "        [0.0352, 0.0235, 0.0345, 0.0436, 0.0410, 0.0288, 0.0232, 0.0336],\n",
      "        [0.0235, 0.0345, 0.0436, 0.0410, 0.0288, 0.0232, 0.0336, 0.0263],\n",
      "        [0.0345, 0.0436, 0.0410, 0.0288, 0.0232, 0.0336, 0.0263, 0.0260],\n",
      "        [0.0436, 0.0410, 0.0288, 0.0232, 0.0336, 0.0263, 0.0260, 0.0428],\n",
      "        [0.0410, 0.0288, 0.0232, 0.0336, 0.0263, 0.0260, 0.0428, 0.0339],\n",
      "        [0.0288, 0.0232, 0.0336, 0.0263, 0.0260, 0.0428, 0.0339, 0.0231],\n",
      "        [0.0232, 0.0336, 0.0263, 0.0260, 0.0428, 0.0339, 0.0231, 0.0228],\n",
      "        [0.0336, 0.0263, 0.0260, 0.0428, 0.0339, 0.0231, 0.0228, 0.0262],\n",
      "        [0.0263, 0.0260, 0.0428, 0.0339, 0.0231, 0.0228, 0.0262, 0.0336],\n",
      "        [0.0260, 0.0428, 0.0339, 0.0231, 0.0228, 0.0262, 0.0336, 0.0232],\n",
      "        [0.0428, 0.0339, 0.0231, 0.0228, 0.0262, 0.0336, 0.0232, 0.0460],\n",
      "        [0.0339, 0.0231, 0.0228, 0.0262, 0.0336, 0.0232, 0.0460, 0.0564],\n",
      "        [0.0231, 0.0228, 0.0262, 0.0336, 0.0232, 0.0460, 0.0564, 0.0326],\n",
      "        [0.0228, 0.0262, 0.0336, 0.0232, 0.0460, 0.0564, 0.0326, 0.0235],\n",
      "        [0.0262, 0.0336, 0.0232, 0.0460, 0.0564, 0.0326, 0.0235, 0.0231],\n",
      "        [0.0336, 0.0232, 0.0460, 0.0564, 0.0326, 0.0235, 0.0231, 0.1152],\n",
      "        [0.0232, 0.0460, 0.0564, 0.0326, 0.0235, 0.0231, 0.1152, 0.2201],\n",
      "        [0.0460, 0.0564, 0.0326, 0.0235, 0.0231, 0.1152, 0.2201, 0.1093],\n",
      "        [0.0564, 0.0326, 0.0235, 0.0231, 0.1152, 0.2201, 0.1093, 0.1034],\n",
      "        [0.0326, 0.0235, 0.0231, 0.1152, 0.2201, 0.1093, 0.1034, 0.0749],\n",
      "        [0.0235, 0.0231, 0.1152, 0.2201, 0.1093, 0.1034, 0.0749, 0.0659],\n",
      "        [0.0231, 0.1152, 0.2201, 0.1093, 0.1034, 0.0749, 0.0659, 0.0766],\n",
      "        [0.1152, 0.2201, 0.1093, 0.1034, 0.0749, 0.0659, 0.0766, 0.0805],\n",
      "        [0.2201, 0.1093, 0.1034, 0.0749, 0.0659, 0.0766, 0.0805, 0.0920],\n",
      "        [0.1093, 0.1034, 0.0749, 0.0659, 0.0766, 0.0805, 0.0920, 0.0765],\n",
      "        [0.1034, 0.0749, 0.0659, 0.0766, 0.0805, 0.0920, 0.0765, 0.0900],\n",
      "        [0.0749, 0.0659, 0.0766, 0.0805, 0.0920, 0.0765, 0.0900, 0.0648],\n",
      "        [0.0659, 0.0766, 0.0805, 0.0920, 0.0765, 0.0900, 0.0648, 0.1429],\n",
      "        [0.0766, 0.0805, 0.0920, 0.0765, 0.0900, 0.0648, 0.1429, 0.0365],\n",
      "        [0.0805, 0.0920, 0.0765, 0.0900, 0.0648, 0.1429, 0.0365, 0.0585],\n",
      "        [0.0920, 0.0765, 0.0900, 0.0648, 0.1429, 0.0365, 0.0585, 0.0530],\n",
      "        [0.0765, 0.0900, 0.0648, 0.1429, 0.0365, 0.0585, 0.0530, 0.0552],\n",
      "        [0.0900, 0.0648, 0.1429, 0.0365, 0.0585, 0.0530, 0.0552, 0.0430],\n",
      "        [0.0648, 0.1429, 0.0365, 0.0585, 0.0530, 0.0552, 0.0430, 0.0391],\n",
      "        [0.1429, 0.0365, 0.0585, 0.0530, 0.0552, 0.0430, 0.0391, 0.0526],\n",
      "        [0.0365, 0.0585, 0.0530, 0.0552, 0.0430, 0.0391, 0.0526, 0.0590],\n",
      "        [0.0585, 0.0530, 0.0552, 0.0430, 0.0391, 0.0526, 0.0590, 0.0448],\n",
      "        [0.0530, 0.0552, 0.0430, 0.0391, 0.0526, 0.0590, 0.0448, 0.0386],\n",
      "        [0.0552, 0.0430, 0.0391, 0.0526, 0.0590, 0.0448, 0.0386, 0.0506],\n",
      "        [0.0430, 0.0391, 0.0526, 0.0590, 0.0448, 0.0386, 0.0506, 0.0409],\n",
      "        [0.0391, 0.0526, 0.0590, 0.0448, 0.0386, 0.0506, 0.0409, 0.0379],\n",
      "        [0.0526, 0.0590, 0.0448, 0.0386, 0.0506, 0.0409, 0.0379, 0.0565],\n",
      "        [0.0590, 0.0448, 0.0386, 0.0506, 0.0409, 0.0379, 0.0565, 0.0730],\n",
      "        [0.0448, 0.0386, 0.0506, 0.0409, 0.0379, 0.0565, 0.0730, 0.0395],\n",
      "        [0.0386, 0.0506, 0.0409, 0.0379, 0.0565, 0.0730, 0.0395, 0.0275],\n",
      "        [0.0506, 0.0409, 0.0379, 0.0565, 0.0730, 0.0395, 0.0275, 0.0304],\n",
      "        [0.0409, 0.0379, 0.0565, 0.0730, 0.0395, 0.0275, 0.0304, 0.0421],\n",
      "        [0.0379, 0.0565, 0.0730, 0.0395, 0.0275, 0.0304, 0.0421, 0.0395],\n",
      "        [0.0565, 0.0730, 0.0395, 0.0275, 0.0304, 0.0421, 0.0395, 0.0507],\n",
      "        [0.0730, 0.0395, 0.0275, 0.0304, 0.0421, 0.0395, 0.0507, 0.0309],\n",
      "        [0.0395, 0.0275, 0.0304, 0.0421, 0.0395, 0.0507, 0.0309, 0.1291],\n",
      "        [0.0275, 0.0304, 0.0421, 0.0395, 0.0507, 0.0309, 0.1291, 0.0531],\n",
      "        [0.0304, 0.0421, 0.0395, 0.0507, 0.0309, 0.1291, 0.0531, 0.0457],\n",
      "        [0.0421, 0.0395, 0.0507, 0.0309, 0.1291, 0.0531, 0.0457, 0.1025],\n",
      "        [0.0395, 0.0507, 0.0309, 0.1291, 0.0531, 0.0457, 0.1025, 0.1261],\n",
      "        [0.0507, 0.0309, 0.1291, 0.0531, 0.0457, 0.1025, 0.1261, 0.1658],\n",
      "        [0.0309, 0.1291, 0.0531, 0.0457, 0.1025, 0.1261, 0.1658, 0.0836],\n",
      "        [0.1291, 0.0531, 0.0457, 0.1025, 0.1261, 0.1658, 0.0836, 0.1718],\n",
      "        [0.0531, 0.0457, 0.1025, 0.1261, 0.1658, 0.0836, 0.1718, 0.5044]])\n",
      "Labels:\n",
      " tensor([[2.6167e-04, 3.0003e+00, 4.9778e-04, 1.3046e-02, 4.3953e-03, 2.4600e-03],\n",
      "        [2.5778e-04, 2.0003e+00, 5.0167e-04, 2.2845e-02, 4.4803e-03, 2.6267e-03],\n",
      "        [2.5972e-04, 3.0003e+00, 4.9944e-04, 4.4028e-03, 4.3997e-03, 1.8285e-02],\n",
      "        [2.6167e-04, 3.0003e+00, 5.0056e-04, 9.9667e-04, 4.3553e-03, 7.5658e-03],\n",
      "        [2.5556e-04, 2.0003e+00, 4.9917e-04, 1.0019e-03, 4.4075e-03, 2.8467e-03],\n",
      "        [2.6083e-04, 3.0003e+00, 4.9917e-04, 1.3464e-02, 4.4500e-03, 3.0033e-03],\n",
      "        [2.5611e-04, 3.0003e+00, 5.0250e-04, 4.4883e-03, 4.4019e-03, 2.9400e-03],\n",
      "        [2.6083e-04, 2.0003e+00, 5.0000e-04, 3.8906e-03, 4.3464e-03, 2.8000e-03],\n",
      "        [2.6083e-04, 3.0003e+00, 4.9972e-04, 2.2831e-02, 4.3714e-03, 3.0000e-03],\n",
      "        [2.5694e-04, 3.0003e+00, 5.0167e-04, 1.3131e-02, 4.4217e-03, 2.9567e-03],\n",
      "        [2.5694e-04, 2.0003e+00, 4.9889e-04, 1.0022e-03, 1.0671e-02, 2.1867e-03],\n",
      "        [2.5972e-04, 2.0003e+00, 5.0194e-04, 1.0014e-03, 7.8792e-03, 2.8200e-03],\n",
      "        [2.5722e-04, 3.0003e+00, 4.9833e-04, 4.0200e-03, 7.9478e-03, 1.9033e-03],\n",
      "        [2.5250e-04, 2.0003e+00, 5.0056e-04, 1.2922e-02, 4.4767e-03, 1.9433e-03],\n",
      "        [2.5056e-04, 1.0003e+00, 4.9972e-04, 1.0025e-03, 4.4403e-03, 2.0600e-03],\n",
      "        [2.5361e-04, 3.0003e+00, 5.0389e-04, 9.8461e-03, 4.3136e-03, 1.8104e-02],\n",
      "        [2.7000e-04, 3.0003e+00, 5.0000e-04, 2.2522e-02, 4.3664e-03, 1.7729e-02],\n",
      "        [2.5222e-04, 2.0003e+00, 5.0056e-04, 8.4011e-03, 4.4625e-03, 6.8058e-03],\n",
      "        [2.7028e-04, 3.0003e+00, 5.0028e-04, 1.0053e-03, 4.4558e-03, 2.9933e-03],\n",
      "        [2.5583e-04, 2.0003e+00, 5.0083e-04, 9.9694e-04, 4.4992e-03, 2.5067e-03],\n",
      "        [1.1753e-03, 4.0012e+00, 9.9655e-02, 5.9169e-03, 4.2625e-03, 2.5433e-03],\n",
      "        [2.5278e-04, 2.0003e+00, 1.8791e-01, 1.1106e-02, 4.4658e-03, 1.9541e-02],\n",
      "        [4.5007e-02, 3.0450e+00, 2.9683e-02, 1.4766e-02, 4.3706e-03, 1.5786e-02],\n",
      "        [6.4784e-02, 4.0648e+00, 5.0000e-04, 2.2269e-02, 4.3500e-03, 1.6391e-02],\n",
      "        [3.0576e-02, 4.0306e+00, 5.0306e-04, 9.9806e-04, 4.2569e-03, 1.6314e-02],\n",
      "        [2.5583e-04, 2.0003e+00, 5.0056e-04, 1.0011e-03, 4.3978e-03, 1.7430e-02],\n",
      "        [2.5639e-04, 3.0003e+00, 5.0000e-04, 1.1853e-02, 4.1656e-03, 2.0352e-02],\n",
      "        [2.6389e-04, 3.0003e+00, 5.0056e-04, 1.8319e-02, 4.1694e-03, 1.8833e-02],\n",
      "        [2.6472e-04, 3.0003e+00, 5.0667e-04, 3.2097e-02, 4.1717e-03, 1.5204e-02],\n",
      "        [2.5806e-04, 3.0003e+00, 5.0056e-04, 1.2833e-02, 4.2353e-03, 1.7995e-02],\n",
      "        [2.5583e-04, 3.0003e+00, 4.9889e-04, 2.2614e-02, 4.1458e-03, 2.3449e-02],\n",
      "        [2.5333e-04, 3.0003e+00, 5.0139e-04, 2.3479e-02, 4.1494e-03, 2.2027e-02],\n",
      "        [2.5333e-04, 2.0003e+00, 1.0250e-01, 1.3505e-02, 4.1594e-03, 1.5331e-02],\n",
      "        [3.3881e-03, 3.0034e+00, 5.0444e-04, 1.2806e-03, 4.2242e-03, 1.5380e-02],\n",
      "        [1.0856e-02, 4.0109e+00, 5.0028e-04, 1.3934e-02, 4.1431e-03, 1.3769e-02],\n",
      "        [1.2383e-02, 4.0124e+00, 4.9944e-04, 2.3324e-02, 4.1183e-03, 2.5200e-03],\n",
      "        [1.2565e-02, 3.0126e+00, 5.0028e-04, 2.6159e-02, 4.1222e-03, 2.5400e-03],\n",
      "        [1.2561e-02, 3.0126e+00, 5.0139e-04, 1.0671e-02, 4.2042e-03, 3.6036e-03],\n",
      "        [7.9144e-03, 5.0079e+00, 4.9944e-04, 2.4333e-03, 4.1281e-03, 1.2754e-02],\n",
      "        [6.5775e-03, 4.0066e+00, 5.0250e-04, 1.5859e-02, 4.1206e-03, 1.5274e-02],\n",
      "        [6.4811e-03, 4.0065e+00, 5.0000e-04, 2.3461e-02, 4.1656e-03, 1.5055e-02],\n",
      "        [6.4886e-03, 4.0065e+00, 5.0611e-04, 6.7175e-03, 4.3053e-03, 1.5406e-02],\n",
      "        [6.4767e-03, 3.0065e+00, 5.0750e-04, 1.0028e-03, 4.1806e-03, 1.5024e-02],\n",
      "        [5.7836e-03, 4.0058e+00, 5.0111e-04, 1.3450e-02, 4.1011e-03, 1.3701e-02],\n",
      "        [4.7506e-03, 3.0048e+00, 5.0583e-04, 4.1578e-03, 4.1383e-03, 1.4236e-02],\n",
      "        [4.7567e-03, 2.0048e+00, 5.0028e-04, 9.9611e-04, 4.2461e-03, 1.4670e-02],\n",
      "        [4.7667e-03, 2.0048e+00, 4.9917e-04, 2.0374e-02, 4.1661e-03, 1.5069e-02],\n",
      "        [4.7997e-03, 3.0048e+00, 2.5586e-02, 2.2487e-02, 4.1322e-03, 1.1417e-02],\n",
      "        [4.7558e-03, 3.0048e+00, 1.0108e-02, 1.1017e-03, 4.1389e-03, 5.2739e-03],\n",
      "        [4.7750e-03, 3.0048e+00, 5.0056e-04, 1.0011e-03, 4.2425e-03, 2.7133e-03],\n",
      "        [4.7597e-03, 3.0048e+00, 5.0417e-04, 1.0033e-03, 4.1914e-03, 5.0647e-03],\n",
      "        [4.7836e-03, 3.0048e+00, 1.3207e-02, 1.6171e-02, 4.1567e-03, 3.0033e-03],\n",
      "        [4.8042e-03, 3.0048e+00, 5.0139e-04, 1.0844e-02, 4.1822e-03, 4.8817e-03],\n",
      "        [4.7744e-03, 3.0048e+00, 5.0333e-04, 2.2720e-02, 4.2369e-03, 4.7192e-03],\n",
      "        [4.7553e-03, 4.0048e+00, 4.9972e-04, 2.7133e-03, 4.1319e-03, 2.4967e-03],\n",
      "        [4.7858e-03, 3.0048e+00, 9.4771e-02, 1.2167e-03, 4.1089e-03, 1.7783e-02],\n",
      "        [4.7808e-03, 3.0048e+00, 4.9944e-04, 1.0672e-02, 4.1025e-03, 1.9549e-02],\n",
      "        [4.8144e-03, 3.0048e+00, 7.1744e-03, 7.3236e-03, 4.1817e-03, 9.5147e-03],\n",
      "        [8.2576e-02, 4.0826e+00, 4.9944e-04, 5.6072e-03, 4.0672e-03, 4.5242e-03],\n",
      "        [9.5574e-02, 3.0956e+00, 5.0167e-04, 2.3095e-02, 4.0975e-03, 2.3600e-03],\n",
      "        [9.5932e-02, 4.0959e+00, 5.1437e-02, 1.3112e-02, 4.0647e-03, 2.9000e-03],\n",
      "        [6.5960e-02, 4.0660e+00, 5.0083e-04, 1.0050e-03, 4.2000e-03, 3.8603e-03],\n",
      "        [9.3584e-02, 4.0936e+00, 8.1103e-02, 1.5658e-03, 4.1914e-03, 2.2667e-03],\n",
      "        [9.2052e-02, 4.0921e+00, 4.9972e-04, 4.3842e-01, 4.3214e-03, 2.0733e-03]])\n",
      "Shape of features: torch.Size([64, 8])\n",
      "Shape of labels: torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('H1_AI_Dataset/Dataset1_Input.csv', 'H1_AI_Dataset/Dataset1_Labels.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for features, labels in dataloader:  # This correctly unpacks each batch into features and labels\n",
    "    print(\"Features:\\n\", features)\n",
    "    print(\"Labels:\\n\", labels)\n",
    "    print(\"Shape of features:\", features.shape)  \n",
    "    print(\"Shape of labels:\", labels.shape)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fda0df9-5c78-498b-9d6e-ec2d57f2ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b386cdc5-cf2e-4894-97ff-aed03a5c372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(1,128,2,6)\n",
    "criterion = nn.MSELoss()  # For regression; change accordingly if it's a classification task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b55851a0-b88b-4d58-9a7e-3fc013a690f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(1, 128, num_layers=2)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fee34ce-2a9c-4ac1-949b-967a2370cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8])\n",
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[53], line 12\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:803\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    801\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 803\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    804\u001b[0m         hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        print(data.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2113f9-76d1-4ff6-ab78-e348a6d26ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
