{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6597e432-afa7-4ae8-ae43-0c0f25cabc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4273efe-a0bf-4de5-9092-9b745c8a3302",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba93302-b8ce-4113-ac29-7bdfa5d3c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, features_file, label_file):\n",
    "        self.features = pd.read_csv(features_file)\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float)\n",
    "        tensor_labels = torch.tensor(self.labels.iloc[idx].values, dtype=torch.float)\n",
    "\n",
    "        return tensor_features, tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f37718a-e3f0-4b0a-a9d8-8732bb7025bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('H1_AI_Dataset/H1_Features_Wh.csv', 'H1_AI_Dataset/H1_Labels_Wh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7b025-542a-43bf-9047-b06d73ab914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d46697-0b29-4186-b825-a0e3f37dd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9cb80-2f7d-4edc-8ea0-689362ea28c5",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f488f9-8f84-48fb-9383-cc5073542980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=128, output_size=6):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        \n",
    "        x = self.fc5(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32e5d9f-4d0d-41f4-afd8-68398b643533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, optimizer, and loss function\n",
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c097db2a-49ad-428b-b775-59c89f80d2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "787b9190-7288-4f40-8220-68a40bd4f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Max gradient: 1.605897307395935, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 300: Max gradient: 0.5844643712043762, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 600: Max gradient: 1.8740284442901611, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Epoch [1/4], Training Loss: 9.1295\n",
      "Epoch [1/4], Test Loss: 8.2067\n",
      "Batch 0: Max gradient: 1.1917110681533813, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 300: Max gradient: 1.1702181100845337, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 600: Max gradient: 4.928669452667236, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Epoch [2/4], Training Loss: 9.2605\n",
      "Epoch [2/4], Test Loss: 8.2574\n",
      "Batch 0: Max gradient: 2.811589241027832, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 300: Max gradient: 0.7960371971130371, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 600: Max gradient: 0.915511965751648, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Epoch [3/4], Training Loss: 9.2463\n",
      "Epoch [3/4], Test Loss: 8.2476\n",
      "Batch 0: Max gradient: 8.931171417236328, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 300: Max gradient: 1.8302421569824219, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Batch 600: Max gradient: 5.407120227813721, Min gradient: 0.0\n",
      "Warning: Possible vanishing gradients detected.\n",
      "Epoch [4/4], Training Loss: 9.2583\n",
      "Target: [10.759444  18.581667   0.5       23.0725     3.6066666  2.7966666]\n",
      "Output: [ 2.667709    0.7844689   0.56454325 19.798294    3.9467888   3.0784574 ]\n",
      "Target: [ 0.25833333  1.2427778   0.50083333 22.6375      3.8180556   0.        ]\n",
      "Output: [ 3.0298629  0.7576493  0.5544594 16.332926   3.9921641  3.1719987]\n",
      "Target: [ 1.0033333   0.75027776  0.50083333 22.2575      8.161389    3.        ]\n",
      "Output: [ 2.4822712  0.7573696  0.5945447 18.264683   3.9678826  3.144872 ]\n",
      "Target: [ 0.2502778   0.7463889   0.49944445 24.861666    3.7516668   0.5127778 ]\n",
      "Output: [ 2.8084197   0.8120258   0.58659196 20.344934    3.935889    3.1776    ]\n",
      "Target: [ 0.51        0.74833333  0.49944445  6.3780556   7.4888887  15.251667  ]\n",
      "Output: [ 2.0727525  0.730635   0.5276131 17.031998   3.9304597  3.0629485]\n",
      "Target: [ 0.2625      0.74916667  0.4997222  10.162778    3.7166667   7.456111  ]\n",
      "Output: [ 2.5885346   0.76736444  0.54622054 16.693214    4.019061    3.092093  ]\n",
      "Target: [41.768333  43.513054   0.5005556 12.195556   8.792778   3.1152778]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [12.952222    0.7261111   0.49833333 11.708889    3.8027778   3.253611  ]\n",
      "Output: [ 3.4534066   0.72635794  0.5077332  17.534863    4.0105257   3.1104217 ]\n",
      "Target: [0.25694445 0.7416667  0.24916667 0.9980556  3.3219445  1.466389  ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [ 0.5011111  1.2452778  0.5044444  1.0069444  3.5019445 14.453889 ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [35.771946    1.2427778   0.50083333  1.0827777   4.692222   15.128056  ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [ 8.266666   1.2466667  0.5       36.400555   3.5827777 14.651667 ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [44.545      1.2402778 24.991667   9.232778   3.6805556  3.1405556]\n",
      "Output: [ 2.980271   0.8104277  0.5734081 18.693043   3.9757574  3.1970336]\n",
      "Target: [ 0.26777777  1.2516667   0.50027776 13.628612    3.79        0.        ]\n",
      "Output: [ 3.1732974   0.75018954  0.57182235 16.808172    4.0026345   3.087763  ]\n",
      "Target: [31.527779    0.74194443 57.329166   11.373333    5.278333    2.9933333 ]\n",
      "Output: [ 2.7737155   0.8202039   0.58594495 18.570932    3.8953173   3.150504  ]\n",
      "Target: [27.822222   0.7436111 39.051666   4.9947224  3.8302777 18.662777 ]\n",
      "Output: [ 2.472886    0.83814156  0.5865503  18.89453     3.9078567   3.1925352 ]\n",
      "Target: [38.476387    0.73888886  0.49888888 22.60389     3.8441668  14.684444  ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [0.74916667 0.7513889  0.7511111  0.99777776 3.7041667  3.1080556 ]\n",
      "Output: [1.7951832  0.75039005 0.5022187  4.6577473  3.8665984  3.1024218 ]\n",
      "Target: [0.24972223 0.7505556  0.5013889  0.9988889  3.9222221  8.8336115 ]\n",
      "Output: [ 3.4469886   0.71239567  0.5210986  16.195461    3.979788    3.0033581 ]\n",
      "Target: [ 4.7444444   0.73694444  0.49944445 23.341944    3.6575      0.        ]\n",
      "Output: [ 2.9479134   0.71801096  0.53627473 15.385922    3.950476    3.1112628 ]\n",
      "Epoch [4/4], Test Loss: 8.2814\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_dataloader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        if batch_idx % 300 == 0:  # Example: Check every 100 batches\n",
    "            max_grad = 0\n",
    "            min_grad = np.inf\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    # Note: Use param.grad.abs().max() to get the max absolute gradient for this parameter\n",
    "                    max_grad = max(max_grad, param.grad.abs().max().item())\n",
    "                    min_grad = min(min_grad, param.grad.abs().min().item())\n",
    "    \n",
    "            print(f\"Batch {batch_idx}: Max gradient: {max_grad}, Min gradient: {min_grad}\")\n",
    "    \n",
    "            # Optionally, add logic to detect vanishing/exploding gradients\n",
    "            if max_grad > 1e+3:  # Example threshold for exploding gradients\n",
    "                print(\"Warning: Possible exploding gradients detected.\")\n",
    "            if min_grad < 1e-3:  # Example threshold for vanishing gradients\n",
    "                print(\"Warning: Possible vanishing gradients detected.\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "    \n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients for validation data\n",
    "        for batch_idx, (data, targets) in enumerate(test_dataloader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Randomly print data and targets based on percentage - on last epoch\n",
    "            if (epoch == num_epochs-1) and (random.random() < 0.1):\n",
    "                \n",
    "                # Pick a random item index from the batch\n",
    "                item_idx = random.randint(0, data.size(0) - 1)\n",
    "\n",
    "                # Extract and print the data, target, and output for the randomly selected item\n",
    "                item_target = targets[item_idx]\n",
    "                item_output = outputs[item_idx]\n",
    "                \n",
    "                print(f\"Target: {item_target.cpu().numpy()}\")\n",
    "                print(f\"Output: {item_output.cpu().numpy()}\")\n",
    "\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {total_test_loss/len(test_dataloader):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
